{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Introduction to gradients and automatic differentiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6P32iYYV27b"
      },
      "source": [
        "## Automatic Differentiation and Gradients\n",
        "\n",
        "[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
        "is useful for implementing machine learning algorithms such as\n",
        "[backpropagation](https://en.wikipedia.org/wiki/Backpropagation) for training\n",
        "neural networks.\n",
        "\n",
        "In this guide, you will explore ways to compute gradients with TensorFlow, especially in eager execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Computing gradients\n",
        "\n",
        "To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the *forward* pass.  Then, during the *backward pass*, TensorFlow traverses this list of operations in reverse order to compute gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CLWJl0QliB0"
      },
      "source": [
        "## Gradient tapes\n",
        "\n",
        "TensorFlow provides the `tf.GradientTape` API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variable`s.\n",
        "TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using [reverse mode differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
        "\n",
        "Here is a simple example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xq9GgTCP7a4A"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR9tFAP_7cra"
      },
      "source": [
        "Once you've recorded some operations, use `GradientTape.gradient(target, sources)` to calculate the gradient of some target (often a loss) relative to some source (often the model's variables):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LsvrwF6bHroC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "dy_dx.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2_aqsO25Vx1"
      },
      "source": [
        "The above example uses scalars, but `tf.GradientTape` works as easily on any tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vacZ3-Ws5VdV"
      },
      "outputs": [],
      "source": [
        "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x @ w + b\n",
        "  loss = tf.reduce_mean(y**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4eXOkrQ-9Pb"
      },
      "source": [
        "To get the gradient of `loss` with respect to both variables, you can pass both as sources to the `gradient` method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see `tf.nest`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "luOtK1Da_BR0"
      },
      "outputs": [],
      "source": [
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei4iVXi6qgM7"
      },
      "source": [
        "The gradient with respect to each source has the shape of the source:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aYbWRFPZqk4U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 2)\n",
            "(3, 2)\n"
          ]
        }
      ],
      "source": [
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI_SzxHsvao1"
      },
      "source": [
        "Here is the gradient calculation again, this time passing a dictionary of variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d73cY6NOuaMd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 9.447046  , -0.21505377], dtype=float32)>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_vars = {\n",
        "    'w': w,\n",
        "    'b': b\n",
        "}\n",
        "\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "grad['b']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ2LvHifEMgO"
      },
      "source": [
        "## Gradients with respect to a model\n",
        "\n",
        "It's common to collect `tf.Variables` into a `tf.Module` or one of its subclasses (`layers.Layer`, `keras.Model`) for [checkpointing](checkpoint.ipynb) and [exporting](saved_model.ipynb).\n",
        "\n",
        "In most cases, you will want to calculate gradients with respect to a model's trainable variables.  Since all subclasses of `tf.Module` aggregate their variables in the `Module.trainable_variables` property, you can calculate these gradients in a few lines of code: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JvesHtbQESc-"
      },
      "outputs": [],
      "source": [
        "layer = tf.keras.layers.Dense(2, activation='relu')\n",
        "x = tf.constant([[1., 2., 3.]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Forward pass\n",
        "  y = layer(x)\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "\n",
        "# Calculate gradients with respect to every trainable variable\n",
        "grad = tape.gradient(loss, layer.trainable_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PR_ezr6UFrpI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dense/kernel:0, shape: (3, 2)\n",
            "dense/bias:0, shape: (2,)\n"
          ]
        }
      ],
      "source": [
        "for var, g in zip(layer.trainable_variables, grad):\n",
        "  print(f'{var.name}, shape: {g.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6Gx6LS714zR"
      },
      "source": [
        "<a id=\"watches\"></a>\n",
        "\n",
        "## Controlling what the tape watches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4VlqKFzzGaC"
      },
      "source": [
        "The default behavior is to record all operations after accessing a trainable `tf.Variable`. The reasons for this are:\n",
        "\n",
        "* The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n",
        "* The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
        "* The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n",
        "\n",
        "For example, the following fails to calculate a gradient because the `tf.Tensor` is not \"watched\" by default, and the `tf.Variable` is not trainable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Kj9gPckdB37a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "None\n",
            "None\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# A trainable variable\n",
        "x0 = tf.Variable(3.0, name='x0')\n",
        "# Not trainable\n",
        "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
        "# Not a Variable: A variable + tensor returns a tensor.\n",
        "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
        "# Not a variable\n",
        "x3 = tf.constant(3.0, name='x3')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = (x0**2) + (x1**2) + (x2**2)\n",
        "\n",
        "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
        "\n",
        "for g in grad:\n",
        "  print(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkcpQnLgNxgi"
      },
      "source": [
        "You can list the variables being watched by the tape using the `GradientTape.watched_variables` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hwNwjW1eAkib"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['x0:0']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[var.name for var in tape.watched_variables()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB9I1uFvB4tf"
      },
      "source": [
        "`tf.GradientTape` provides hooks that give the user control over what is or is not watched.\n",
        "\n",
        "To record gradients with respect to a `tf.Tensor`, you need to call `GradientTape.watch(x)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "tVN1QqFRDHBK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6.0\n"
          ]
        }
      ],
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x**2\n",
        "\n",
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxsiYnf2DN8K"
      },
      "source": [
        "Conversely, to disable the default behavior of watching all `tf.Variables`, set `watch_accessed_variables=False` when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7QPzwWvSEwIp"
      },
      "outputs": [],
      "source": [
        "x0 = tf.Variable(0.0,trainable=False)\n",
        "x1 = tf.Variable(10.0,trainable=False)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x0)\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.softplus(x1)\n",
        "  y = y0 + y1\n",
        "  ys = tf.reduce_sum(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRduLbE1H2IJ"
      },
      "source": [
        "Since `GradientTape.watch` was not called on `x0`, no gradient is computed with respect to it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "e6GM-3evH1Sz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dy/dx0: 1.0\n",
            "dy/dx1: None\n"
          ]
        }
      ],
      "source": [
        "# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n",
        "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
        "\n",
        "print('dy/dx0:', grad['x0'].numpy())\n",
        "print('dy/dx1:', grad['x1'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g1nKB6P-OnA"
      },
      "source": [
        "## Intermediate results\n",
        "\n",
        "You can also request gradients of the output with respect to intermediate values computed inside the `tf.GradientTape` context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7XaPRAwUyYms"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18.0\n"
          ]
        }
      ],
      "source": [
        "x = tf.constant(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "# Use the tape to compute the gradient of z with respect to the\n",
        "# intermediate value y.\n",
        "# dz_dy = 2 * y and y = x ** 2 = 9\n",
        "print(tape.gradient(z, y).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISkXuY7YzIcS"
      },
      "source": [
        "By default, the resources held by a `GradientTape` are released as soon as the `GradientTape.gradient` method is called. To compute multiple gradients over the same computation, create a gradient tape with `persistent=True`. This allows multiple calls to the `gradient` method as resources are released when the tape object is garbage collected. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "zZaCm3-9zVCi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([0.99999994 0.        ], shape=(2,), dtype=float32)\n",
            "tf.Tensor([0.70710677 1.        ], shape=(2,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "x = tf.constant([np.pi/4, 0])\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  y = tf.math.sin(x)\n",
        "  z = y * y\n",
        "\n",
        "print(tape.gradient(z, x))  # [4.0, 108.0] (4 * x**3 at x = [1.0, 3.0])\n",
        "print(tape.gradient(y, x))  # [2.0, 6.0] (2 * x at x = [1.0, 3.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "j8bv_jQFg6CN"
      },
      "outputs": [],
      "source": [
        "del tape   # Drop the reference to the tape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_ZY-9BUB7vX"
      },
      "source": [
        "## Notes on performance\n",
        "\n",
        "* There is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n",
        "\n",
        "* Gradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n",
        "\n",
        "  For efficiency, some ops (like `ReLU`) don't need to keep their intermediate results and they are pruned during the forward pass. However, if you use `persistent=True` on your tape, *nothing is discarded* and your peak memory usage will be higher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dLBpZsJebFq"
      },
      "source": [
        "## Gradients of non-scalar targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pldU9F5duP2"
      },
      "source": [
        "A gradient is fundamentally an operation on a scalar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qI0sDV_WeXBb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.0\n",
            "-0.25\n"
          ]
        }
      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient(y0, x).numpy())\n",
        "print(tape.gradient(y1, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COEyYp34fxj4"
      },
      "source": [
        "Thus, if you ask for the gradient of multiple targets, the result for each source is:\n",
        "\n",
        "* The gradient of the sum of the targets, or equivalently\n",
        "* The sum of the gradients of each target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "o4a6_YOcfWKS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.75\n"
          ]
        }
      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvP-mkBMgbym"
      },
      "source": [
        "Similarly, if the target(s) are not scalar the gradient of the sum is calculated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DArPWqsSh5un"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.0\n"
          ]
        }
      ],
      "source": [
        "x = tf.Variable(2.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x * [3., 4.]\n",
        "\n",
        "print(tape.gradient(y, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flDbx68Zh5Lb"
      },
      "source": [
        "This makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\n",
        "\n",
        "If you need a separate gradient for each item, refer to [Jacobians](advanced_autodiff.ipynb#jacobians)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwFswok8RAly"
      },
      "source": [
        "In some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "JQvk_jnMmTDS"
      },
      "outputs": [],
      "source": [
        "x = tf.linspace(-10.0, 5.0, 200+1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = tf.math.exp(x)\n",
        "\n",
        "dy_dx = tape.gradient(y, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "e_f2QgDPmcPE"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(x, y, label='y')\n",
        "plt.plot(x, dy_dx, label='dy/dx')\n",
        "plt.legend()\n",
        "_ = plt.xlabel('x')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kADybtQzYj4"
      },
      "source": [
        "## Control flow\n",
        "\n",
        "Because a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, `if` and `while` statements).\n",
        "\n",
        "Here a different variable is used on each branch of an `if`. The gradient only connects to the variable that was used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ciFLizhrrjy7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "x = tf.constant(1.0)\n",
        "\n",
        "v0 = tf.Variable(2.0)\n",
        "v1 = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  if x > 0.0:\n",
        "    result = v0\n",
        "  else:\n",
        "    result = v1**2 \n",
        "\n",
        "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
        "\n",
        "print(dv0)\n",
        "print(dv1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKnLaiapsjeP"
      },
      "source": [
        "Just remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\n",
        "\n",
        "Depending on the value of `x` in the above example, the tape either records `result = v0` or `result = v1**2`. The gradient with respect to `x` is always `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "8k05WmuAwPm7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "dx = tape.gradient(result, x)\n",
        "\n",
        "print(dx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egypBxISAHhx"
      },
      "source": [
        "## Cases where `gradient` returns `None`\n",
        "\n",
        "When a target is not connected to a source, `gradient` will return `None`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "CU185WDM81Ut"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "x = tf.Variable(2.)\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y * y\n",
        "print(tape.gradient(z, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZbKpHfBRJym"
      },
      "source": [
        "Here `z` is obviously not connected to `x`, but there are several less-obvious ways that a gradient can be disconnected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHDzDOiQ8xmw"
      },
      "source": [
        "### 1. Replaced a variable with a tensor\n",
        "\n",
        "In the section on [\"controlling what the tape watches\"](#watches) you saw that the tape will automatically watch a `tf.Variable` but not a `tf.Tensor`.\n",
        "\n",
        "One common error is to inadvertently replace a `tf.Variable` with a `tf.Tensor`, instead of using `Variable.assign` to update the `tf.Variable`. Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "QPKY4Tn9zX7_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n",
            "EagerTensor : None\n"
          ]
        }
      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "\n",
        "for epoch in range(2):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y = x+1\n",
        "\n",
        "  print(type(x).__name__, \":\", tape.gradient(y, x))\n",
        "  x = x + 1   # This should be `x.assign_add(1)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gwZKxgA97an"
      },
      "source": [
        "### 2. Did calculations outside of TensorFlow\n",
        "\n",
        "The tape can't record the gradient path if the calculation exits TensorFlow.\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jmoLCDJb_yw1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "x = tf.Variable([[1.0, 2.0],\n",
        "                 [3.0, 4.0]], dtype=tf.float32)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  x2 = x**2\n",
        "\n",
        "  # This step is calculated with NumPy\n",
        "  y = np.mean(x2, axis=0)\n",
        "\n",
        "  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
        "  # using `tf.convert_to_tensor`.\n",
        "  y = tf.reduce_mean(y, axis=0)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3YVfP3R-tp7"
      },
      "source": [
        "### 3. Took gradients through an integer or string\n",
        "\n",
        "Integers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n",
        "\n",
        "Nobody expects strings to be differentiable, but it's easy to accidentally create an `int` constant or variable if you don't specify the `dtype`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "9jlHXHqfASU3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "x = tf.constant(10)\n",
        "\n",
        "with tf.GradientTape() as g:\n",
        "  g.watch(x)\n",
        "  y = x * x\n",
        "\n",
        "print(g.gradient(y, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsdP_mTHX9L1"
      },
      "source": [
        "TensorFlow doesn't automatically cast between types, so, in practice, you'll often get a type error instead of a missing gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyAZ7C8qCEs6"
      },
      "source": [
        "### 4. Took gradients through a stateful object\n",
        "\n",
        "State stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n",
        "\n",
        "A `tf.Tensor` is immutable. You can't change a tensor once it's created. It has a _value_, but no _state_. All the operations discussed so far are also stateless: the output of a `tf.matmul` only depends on its inputs.\n",
        "\n",
        "A `tf.Variable` has internal state—its value. When you use the variable, the state is read. It's normal to calculate a gradient with respect to a variable, but the variable's state blocks gradient calculations from going farther back. For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "C1tLeeRFE479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "x0 = tf.Variable(3.0)\n",
        "x1 = tf.Variable(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Update x1 = x1 + x0.\n",
        "  x1.assign_add(x0)\n",
        "  # The tape starts recording from x1.\n",
        "  y = x1**2   # y = (x1 + x0)**2\n",
        "\n",
        "# This doesn't work.\n",
        "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKA92-dqF2r-"
      },
      "source": [
        "Similarly, `tf.data.Dataset` iterators and `tf.queue`s are stateful, and will stop all gradients on tensors that pass through them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHvcDGIbOj2I"
      },
      "source": [
        "## No gradient registered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoc-A6AxVqry"
      },
      "source": [
        "Some `tf.Operation`s are **registered as being non-differentiable** and will return `None`. Others have **no gradient registered**.\n",
        "\n",
        "The `tf.raw_ops` page shows which low-level ops have gradients registered.\n",
        "\n",
        "If you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning `None`. This way you know something has gone wrong.\n",
        "\n",
        "For example, the `tf.image.adjust_contrast` function wraps `raw_ops.AdjustContrastv2`, which could have a gradient but the gradient is not implemented:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HSb20FXc_V0U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LookupError: gradient registry has no entry for: AdjustContrastv2\n"
          ]
        }
      ],
      "source": [
        "image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
        "delta = tf.Variable(0.1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  new_image = tf.image.adjust_contrast(image, delta)\n",
        "\n",
        "try:\n",
        "  print(tape.gradient(new_image, [image, delta]))\n",
        "  assert False   # This should not happen.\n",
        "except LookupError as e:\n",
        "  print(f'{type(e).__name__}: {e}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDoutjzATiEm"
      },
      "source": [
        "If you need to differentiate through this op, you'll either need to implement the gradient and register it (using `tf.RegisterGradient`) or re-implement the function using other ops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCTwc_dQXp2W"
      },
      "source": [
        "## Zeros instead of None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYDrVogA89eA"
      },
      "source": [
        "In some cases it would be convenient to get 0 instead of `None` for unconnected gradients.  You can decide what to return when you have unconnected gradients using the `unconnected_gradients` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "U6zxk1sf9Ixx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "x = tf.Variable([2., 2.])\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y**2\n",
        "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "autodiff.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
